{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Открываем файл с текстом:"
      ],
      "metadata": {
        "id": "vve6Cpxz8BQk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Yz0R06ZvSU0S"
      },
      "outputs": [],
      "source": [
        "with open ('korosteleva_carmarthen.txt') as f:\n",
        "  data = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы знаем, что истории внутри книги разделены тремя звёздочками. А еще обрежем первые 43 символа -- это название книги и автор. Обрежем также примечания к тексту -- они начинаются после фразы \"Август 2000 г.\""
      ],
      "metadata": {
        "id": "GkhV9_G78FPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chapters = data[44:].split('* * *')\n",
        "chapters[-1] = chapters[-1].split('Август 2000 г')[0]"
      ],
      "metadata": {
        "id": "PZIZR0NQVYV9"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Извлечение имен\n",
        "Теперь для каждой истории извлечем все имена, которые там встречаются"
      ],
      "metadata": {
        "id": "zI1eR3_5V9nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для этого воспользуемся лингвистической библиотекой **Natasha**. Наташа -- безумно умная библиотека, которая в числе прочего умеет извлекать из текста именованные сущности -- названия людей, географический объектов и так далее.\n",
        "\n",
        "\n",
        "> Наташа -- это нейросетевая модель, обученная на новостях с размеченными именованными сущностями; поэтому она отлично справляется с новостями, но и с другими жанрами она справляется тоже неплохо.\n",
        "\n",
        "\n",
        "*(Хотя с нашими валлийскими и не только именами она помучается..))))*"
      ],
      "metadata": {
        "id": "2osvk-Qv8lco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сначала технические шаги:\n",
        "* устанавливаем библиотеку\n",
        "* импортируем нужные блоки\n"
      ],
      "metadata": {
        "id": "b7Q97l1b9dDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vRyWgQD-WI7S",
        "outputId": "5d1ea6de-ecec-43e6-9be2-7a2bd0974d5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.26.4)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree>=3->ipymarkup>=0.8.0->natasha)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=ffdb767a27f6d38c76e8ad602928706da87d211142186e9753f0b3dfe275dd7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=503f8a19bceaad4fc4a1a6ba2ea0393e9936de5fd00b6586fa65e5aea01ce125\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: sortedcontainers, razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 sortedcontainers-2.4.0 yargy-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsNERTagger,\n",
        "\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "morph_vocab = MorphVocab()\n",
        "names_extractor = NamesExtractor(morph_vocab)"
      ],
      "metadata": {
        "id": "Xsf0uAygWQaQ"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, как Наташа извлекает имена"
      ],
      "metadata": {
        "id": "Z1fTbL66989s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Афина приехала в подмосковное Пущино на школу Сова. Она знакомится с библиотекой Наташа'\n",
        "doc = Doc(text)\n",
        "\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "\n",
        "doc.tag_ner(ner_tagger)\n",
        "for span in doc.spans:\n",
        "    print(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbHvn0roWclZ",
        "outputId": "f5ff1a43-1886-4180-de47-7221393c3883"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DocSpan(stop=5, type='PER', text='Афина', tokens=[...])\n",
            "DocSpan(start=30, stop=36, type='LOC', text='Пущино', tokens=[...])\n",
            "DocSpan(start=46, stop=50, type='LOC', text='Сова', tokens=[...])\n",
            "DocSpan(start=81, stop=87, type='PER', text='Наташа', tokens=[...])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальше определим функцию, которая берет главу, и извлекает из нее названия персонажей"
      ],
      "metadata": {
        "id": "_LyW3izm_lcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "62KpHZvnhDM0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NER(text):\n",
        "    '''\n",
        "    функция, извлекающая имена\n",
        "    input: глава\n",
        "    output: список имен в этой главе\n",
        "    '''\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "    for s in doc.spans:\n",
        "        s.tokens[-1].lemmatize(morph_vocab)\n",
        "    return [(s.tokens[-1].lemma, s.type) for s in doc.spans]\n",
        "\n",
        "\n",
        "def get_names(book):\n",
        "    '''\n",
        "    функция, которая перебирает главы и записывает, сколько раз персонажи встретились вместе в одной главе\n",
        "    input: list глав\n",
        "    output: словарь имен и словарь связей\n",
        "    '''\n",
        "    persons = {} # все имена\n",
        "    connect = {} # связи\n",
        "\n",
        "    for art in tqdm(book):\n",
        "        nfacts = NER(art) # извлекаем имена\n",
        "        # Так как один участник может упоминаться несколько раз, строим список с единственными упоминаниями.\n",
        "        nam = [fact[0].split(\" \")[-1] for fact in nfacts if fact[1] == 'PER']\n",
        "        snam = list(set(nam))\n",
        "        # Пробрасываем связи между людьми. Главное - не писать сколько раз человек связан между собой.\n",
        "        for n in snam:\n",
        "            persons[n] = persons.get(n, 0) + 1\n",
        "            pers = connect.get(n, {})\n",
        "            for n2 in snam:\n",
        "                if n != n2:\n",
        "                    pers[n2] = pers.get(n2, 0)+1\n",
        "            connect[n] = pers\n",
        "    return persons, connect"
      ],
      "metadata": {
        "id": "ytAWWArzgp8w"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечем связи из книги:"
      ],
      "metadata": {
        "id": "pV0omkZxFCV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names, connections = get_names(chapters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqSHrjI1gvDG",
        "outputId": "e7493f6c-6985-401a-cd02-ca9ad1e12260"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 211/211 [00:49<00:00,  4.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теперь давайте введем несколько ограничений:\n",
        "* оставим только самые частотные имена -- такие, которые встречаются в целом по книге больше 10 раз (`names[n2] > 10`)\n",
        "* оставим только персонажей, \"взаимодействовавших\" (то есть встречавшихся в одной главе) больше пяти раз (`connections[n][n2] > 5`)\n",
        "* удалим всякие перлы Наташи: например, она лемматизирует Змейка как *змейка*, а Керидвен -- как *керидвена*; а еще считает, что скобка -- это имя собственное (`n2 != ')' and n2 != 'змейка' and n2 != 'керидвена'`)\n",
        "\n"
      ],
      "metadata": {
        "id": "OqpEZGOkFG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pers2 = {\n",
        "    n: {\n",
        "        n2: connections[n][n2]\n",
        "        for n2 in connections[n].keys()\n",
        "        if connections[n][n2] > 5 and names[n2] > 10 and n2 != ')' and n2 != 'змейка' and n2 != 'керидвена'\n",
        "    }\n",
        "    for n in connections.keys()\n",
        "    if names[n] > 10 and n != ')' and n != 'змейка' and n != 'керидвена'\n",
        "}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xe4QqlnZiDAX"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pers2"
      ],
      "metadata": {
        "id": "T_c5iWl0G2c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И запишем все в CSV-файл, чтобы потом скормить его Gephi."
      ],
      "metadata": {
        "id": "BrWmBjQ_GYSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('character_interactions.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Source', 'Target', 'Weight'])  # такие заголовки требуются для gephi\n",
        "\n",
        "    for person1, interactions in pers2.items():\n",
        "        for person2, weight in interactions.items():\n",
        "            writer.writerow([person1, person2, weight])"
      ],
      "metadata": {
        "id": "xW3y-XF_ia73"
      },
      "execution_count": 140,
      "outputs": []
    }
  ]
}